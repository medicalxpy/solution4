{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/volume1/home/pxie/.local/lib/python3.9/site-packages/cupy/_environment.py:437: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy-cuda101, cupy-cuda117\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from model import CTM\n",
    "from dataset import CTMDataset\n",
    "from utils.cell_embedding import generate_cell_embedding as generate_cell_embedding_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    设置随机种子，确保实验可重复性\n",
    "\n",
    "    参数:\n",
    "        seed: 随机种子\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # 如果使用多GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(adata_path, gene_embedding_path, device, cell_embed_epochs=300, cell_embed_lr=1e-4):\n",
    "    \"\"\"\n",
    "    准备数据集\n",
    "\n",
    "    参数:\n",
    "        adata_path: h5ad文件路径\n",
    "        gene_embedding_path: 基因嵌入文件路径\n",
    "        device: 使用的设备\n",
    "        cell_embed_epochs: 细胞嵌入训练轮数\n",
    "        cell_embed_lr: 细胞嵌入学习率\n",
    "\n",
    "    返回:\n",
    "        train_dataset: 训练数据集\n",
    "        gene_counts_common: 基因计数\n",
    "        cell_embeddings: 细胞嵌入\n",
    "    \"\"\"\n",
    "    print(f\"正在处理数据集: {adata_path}\")\n",
    "\n",
    "    # 生成细胞嵌入\n",
    "    print(\"生成细胞嵌入...\")\n",
    "    cell_embeddings, gene_counts_common, _ = generate_cell_embedding_new(\n",
    "        adata_path=adata_path,\n",
    "        gene_embedding_path=gene_embedding_path\n",
    "    )\n",
    "\n",
    "    # 准备训练数据集\n",
    "    num_genes = gene_counts_common.n_vars\n",
    "    id2token = {i: list(gene_counts_common.var_names)[i] for i in range(num_genes)}\n",
    "    cell_embeddings_numpy = cell_embeddings.detach().numpy()\n",
    "\n",
    "    train_dataset = CTMDataset(\n",
    "        X_contextual=cell_embeddings_numpy,\n",
    "        X_bow=gene_counts_common.X.toarray(),\n",
    "        idx2token=id2token\n",
    "    )\n",
    "\n",
    "    print(f\"数据集准备完成，基因数: {num_genes}, 细胞数: {len(cell_embeddings)}\")\n",
    "\n",
    "    return train_dataset, gene_counts_common, cell_embeddings\n",
    "\n",
    "def train_model(train_dataset, cell_embeddings, num_topics, batch_size, lr, num_epochs,\n",
    "                patience, device, prior_mean=None, prior_variance=None, seed=42):\n",
    "    \"\"\"\n",
    "    训练主题模型\n",
    "\n",
    "    参数:\n",
    "        train_dataset: 训练数据集\n",
    "        cell_embeddings: 细胞嵌入\n",
    "        num_topics: 主题数量\n",
    "        batch_size: 批次大小\n",
    "        lr: 学习率\n",
    "        num_epochs: 训练轮数\n",
    "        patience: 早停耐心值\n",
    "        device: 使用的设备\n",
    "        prior_mean: 先验均值\n",
    "        prior_variance: 先验方差\n",
    "        seed: 随机种子\n",
    "\n",
    "    返回:\n",
    "        ctm: 训练好的模型\n",
    "        prior_mean: 更新后的先验均值\n",
    "        prior_variance: 更新后的先验方差\n",
    "    \"\"\"\n",
    "    print(\"开始训练主题模型...\")\n",
    "\n",
    "    # 创建模型\n",
    "    ctm = CTM(\n",
    "        contextual_size=cell_embeddings.shape[1],\n",
    "        bow_size=train_dataset.X_bow.shape[1],\n",
    "        n_components=num_topics,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        lr=lr,\n",
    "        num_epochs=num_epochs,\n",
    "        prior_mean=prior_mean,\n",
    "        prior_variance=prior_variance,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    prior_mean, prior_variance = ctm.fit(\n",
    "        train_dataset,\n",
    "        return_mean=False,\n",
    "        patience=patience,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    print(\"模型训练完成\")\n",
    "\n",
    "    return ctm, prior_mean, prior_variance\n",
    "\n",
    "def save_results(ctm, train_dataset, gene_counts_common, model_name, train_order, data_name,\n",
    "                num_topics, output_dir, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    保存训练结果\n",
    "\n",
    "    参数:\n",
    "        ctm: 训练好的模型\n",
    "        train_dataset: 训练数据集\n",
    "        gene_counts_common: 基因计数\n",
    "        model_name: 模型名称\n",
    "        train_order: 训练顺序\n",
    "        data_name: 数据集名称\n",
    "        num_topics: 主题数量\n",
    "        output_dir: 输出目录\n",
    "        checkpoint_dir: 检查点目录\n",
    "    \"\"\"\n",
    "    print(f\"保存模型和结果: {model_name}_{train_order}_{data_name}\")\n",
    "\n",
    "    # 保存模型检查点\n",
    "    checkpoint_name = f\"{model_name}_{train_order}_{data_name}\"\n",
    "    ctm.save(model_dir=checkpoint_dir, part_name=checkpoint_name)\n",
    "\n",
    "    # 获取主题分布\n",
    "    topics_per_cell = ctm.get_thetas(train_dataset)\n",
    "    df_topics_per_cell = pd.DataFrame(topics_per_cell)\n",
    "\n",
    "    # 保存细胞-主题矩阵\n",
    "    cell_output_file = os.path.join(output_dir, f\"{model_name}_{train_order}_{data_name}_t{num_topics}_c.csv\")\n",
    "    df_topics_per_cell.to_csv(cell_output_file, index=False)\n",
    "    print(f\"细胞-主题矩阵已保存至: {cell_output_file}\")\n",
    "\n",
    "    # 获取主题-基因矩阵\n",
    "    topics_per_gene = ctm.get_topic_word_matrix()\n",
    "    df_topics_per_gene = pd.DataFrame(topics_per_gene)\n",
    "    df_gene_topic = df_topics_per_gene.T\n",
    "    df_gene_topic.index = gene_counts_common.var_names\n",
    "\n",
    "    # 保存基因-主题矩阵\n",
    "    gene_output_file = os.path.join(output_dir, f\"{model_name}_{train_order}_{data_name}_t{num_topics}_g.csv\")\n",
    "    df_gene_topic.to_csv(gene_output_file)\n",
    "    print(f\"基因-主题矩阵已保存至: {gene_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理数据集: /volume1/home/pxie/data/PBMC.h5ad\n",
      "生成细胞嵌入...\n",
      "AnnData中的基因数: 36263\n",
      "基因嵌入中的基因数: 20271\n",
      "共有基因数: 19132\n",
      "数据集准备完成，基因数: 19132, 细胞数: 66985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prior_mean = 0.0\n",
    "prior_variance = None\n",
    "train_dataset, gene_counts_common, cell_embeddings = prepare_dataset(\n",
    "    adata_path=\"/volume1/home/pxie/data/PBMC.h5ad\",\n",
    "    gene_embedding_path=\"/volume1/home/pxie/data/embeddings/fused_geneformerv2_genePT.pkl\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练主题模型...\n",
      "Settings: \n",
      "                N Components: 50\n",
      "                Topic Prior Mean: 0.0\n",
      "                Topic Prior Variance: None\n",
      "                Model Type: prodLDA\n",
      "                Hidden Sizes: (100, 100)\n",
      "                Activation: softplus\n",
      "                Dropout: 0.2\n",
      "                Learn Priors: True\n",
      "                Learning Rate: 0.002\n",
      "                Momentum: 0.99\n",
      "                Reduce On Plateau: False\n",
      "                Save Dir: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 4390.235614483173 best_loss: 4390.235614483173\n",
      "epoch: 1 loss: 4337.835201322116 best_loss: 4337.835201322116\n",
      "epoch: 2 loss: 4331.097896634616 best_loss: 4331.097896634616\n",
      "epoch: 3 loss: 4326.9749399038465 best_loss: 4326.9749399038465\n",
      "epoch: 4 loss: 4323.48251953125 best_loss: 4323.48251953125\n",
      "epoch: 5 loss: 4321.752411358173 best_loss: 4321.752411358173\n",
      "epoch: 6 loss: 4319.4267578125 best_loss: 4319.4267578125\n",
      "epoch: 7 loss: 4319.096221454327 best_loss: 4319.096221454327\n",
      "epoch: 8 loss: 4318.141180889423 best_loss: 4318.141180889423\n",
      "epoch: 9 loss: 4315.889926382211 best_loss: 4315.889926382211\n",
      "epoch: 10 loss: 4314.270395132212 best_loss: 4314.270395132212\n",
      "epoch: 11 loss: 4296.521146334135 best_loss: 4296.521146334135\n",
      "epoch: 12 loss: 4278.837154447116 best_loss: 4278.837154447116\n",
      "epoch: 13 loss: 4261.7213792067305 best_loss: 4261.7213792067305\n",
      "epoch: 14 loss: 4246.530506310096 best_loss: 4246.530506310096\n",
      "epoch: 15 loss: 4242.146213942307 best_loss: 4242.146213942307\n",
      "epoch: 16 loss: 4238.694275841346 best_loss: 4238.694275841346\n",
      "epoch: 17 loss: 4235.333661358173 best_loss: 4235.333661358173\n",
      "epoch: 18 loss: 4233.501404747596 best_loss: 4233.501404747596\n",
      "epoch: 19 loss: 4231.549346454327 best_loss: 4231.549346454327\n",
      "epoch: 20 loss: 4230.555558894231 best_loss: 4230.555558894231\n",
      "epoch: 21 loss: 4229.345988581731 best_loss: 4229.345988581731\n",
      "epoch: 22 loss: 4228.900255408654 best_loss: 4228.900255408654\n",
      "epoch: 23 loss: 4228.491654146635 best_loss: 4228.491654146635\n",
      "epoch: 24 loss: 4227.493509615385 best_loss: 4227.493509615385\n",
      "epoch: 25 loss: 4226.642044771635 best_loss: 4226.642044771635\n",
      "epoch: 26 loss: 4226.935329026443 best_loss: 4226.642044771635\n",
      "epoch: 27 loss: 4226.36279296875 best_loss: 4226.36279296875\n",
      "epoch: 28 loss: 4225.893359375 best_loss: 4225.893359375\n",
      "epoch: 29 loss: 4226.596078725962 best_loss: 4225.893359375\n",
      "epoch: 30 loss: 4226.743644831731 best_loss: 4225.893359375\n",
      "epoch: 31 loss: 4226.481009615384 best_loss: 4225.893359375\n",
      "epoch: 32 loss: 4225.904499699519 best_loss: 4225.893359375\n",
      "epoch: 33 loss: 4226.503425480769 best_loss: 4225.893359375\n",
      "epoch: 34 loss: 4225.852546574519 best_loss: 4225.852546574519\n",
      "epoch: 35 loss: 4226.285719651442 best_loss: 4225.852546574519\n",
      "epoch: 36 loss: 4225.391856971154 best_loss: 4225.391856971154\n",
      "epoch: 37 loss: 4224.637229567307 best_loss: 4224.637229567307\n",
      "epoch: 38 loss: 4223.402291165866 best_loss: 4223.402291165866\n",
      "epoch: 39 loss: 4223.412229567308 best_loss: 4223.402291165866\n",
      "epoch: 40 loss: 4224.066789362981 best_loss: 4223.402291165866\n",
      "epoch: 41 loss: 4222.643442007212 best_loss: 4222.643442007212\n",
      "epoch: 42 loss: 4221.687349759615 best_loss: 4221.687349759615\n",
      "epoch: 43 loss: 4222.865993088943 best_loss: 4221.687349759615\n",
      "epoch: 44 loss: 4220.286711237981 best_loss: 4220.286711237981\n",
      "epoch: 45 loss: 4219.434675480769 best_loss: 4219.434675480769\n",
      "epoch: 46 loss: 4219.917255108173 best_loss: 4219.434675480769\n",
      "epoch: 47 loss: 4221.27978515625 best_loss: 4219.434675480769\n",
      "epoch: 48 loss: 4220.4509840745195 best_loss: 4219.434675480769\n",
      "epoch: 49 loss: 4220.985501802885 best_loss: 4219.434675480769\n",
      "epoch: 50 loss: 4219.967750901443 best_loss: 4219.434675480769\n",
      "epoch: 51 loss: 4219.840594951923 best_loss: 4219.434675480769\n",
      "epoch: 52 loss: 4219.989332932692 best_loss: 4219.434675480769\n",
      "epoch: 53 loss: 4218.874045973557 best_loss: 4218.874045973557\n",
      "epoch: 54 loss: 4217.027223557692 best_loss: 4217.027223557692\n",
      "epoch: 55 loss: 4216.721574519231 best_loss: 4216.721574519231\n",
      "epoch: 56 loss: 4216.01845703125 best_loss: 4216.01845703125\n",
      "epoch: 57 loss: 4216.337131911057 best_loss: 4216.01845703125\n",
      "epoch: 58 loss: 4215.437229567307 best_loss: 4215.437229567307\n",
      "epoch: 59 loss: 4215.645184795673 best_loss: 4215.437229567307\n",
      "epoch: 60 loss: 4214.8985952524035 best_loss: 4214.8985952524035\n",
      "epoch: 61 loss: 4214.2693434495195 best_loss: 4214.2693434495195\n",
      "epoch: 62 loss: 4213.345703125 best_loss: 4213.345703125\n",
      "epoch: 63 loss: 4211.842044771634 best_loss: 4211.842044771634\n",
      "epoch: 64 loss: 4210.378665865384 best_loss: 4210.378665865384\n",
      "epoch: 65 loss: 4208.194764122596 best_loss: 4208.194764122596\n"
     ]
    }
   ],
   "source": [
    "ctm, prior_mean, prior_variance = train_model(\n",
    "    train_dataset=train_dataset,\n",
    "    cell_embeddings=cell_embeddings,\n",
    "    num_topics=50,\n",
    "    batch_size=1024,\n",
    "    lr=2e-3,\n",
    "    num_epochs=300,\n",
    "    patience=10,\n",
    "    device=\"cuda:0\",\n",
    "    prior_mean=prior_mean,\n",
    "    prior_variance=prior_variance,\n",
    "    seed=3407\n",
    ")\n",
    "\n",
    "# 保存结果\n",
    "save_results(\n",
    "    ctm=ctm,\n",
    "    train_dataset=train_dataset,\n",
    "    gene_counts_common=gene_counts_common,\n",
    "    model_name='fusion_gpt_gfv2',\n",
    "    train_order=1,\n",
    "    data_name='PBMC',\n",
    "    num_topics=50,\n",
    "    output_dir=\"/volume1/home/pxie/topic_model/solution4/results\",\n",
    "    checkpoint_dir=\"/volume1/home/pxie/topic_model/solution4/results/checkpoint\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
